{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vicliu2001/ECE421_Intro_to_ML/blob/main/Copy_of_ECE421fall24_A4_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR3rYITLNTOe"
   },
   "source": [
    "# Welcome to Assignment 4 -- Part 1\n",
    "\n",
    "In this part of the assignment, wou will train a simple Vanilla RNN and LSTM model on Adding Problem using `PyTorch`\n",
    "\n",
    "This file is a Jupyter Notebook. You can double-click on section headers to show code and run each section with Shift+Enter.\n",
    "\n",
    "\n",
    "**IMPORTANT:** You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XLf2iQ_HxmBu",
    "outputId": "6a81898e-6211-4a2d-df19-4431cee547fb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title ## 0.1 Mounting your Drive and setting up mount symlink.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#@markdown By executing this cell, you will mount your Google Drive, set\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#@markdown by default to prevent Colab instance timeouts from deleting your\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#@markdown edits.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[1;32m     14\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "#@title ## 0.1 Mounting your Drive and setting up mount symlink.\n",
    "\n",
    "#@markdown By executing this cell, you will mount your Google Drive, set\n",
    "#@markdown up mount symlink, and apt install requirements.\n",
    "\n",
    "#@markdown Your work will be stored in a folder called `ece421_f2024_A4_folder`\n",
    "#@markdown by default to prevent Colab instance timeouts from deleting your\n",
    "#@markdown edits.\n",
    "\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "from importlib import reload\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "## setting the path parameters and creating the folder if needed\n",
    "DRIVE_PATH = '/content/gdrive/MyDrive/ece421_f2024_A4_folder'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "  %mkdir $DRIVE_PATH\n",
    "\n",
    "## make a symlink\n",
    "SYM_PATH = '/content/ece421_f2024_A4_folder'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "  !ln -s $DRIVE_PATH $SYM_PATH\n",
    "\n",
    "!apt install the requirements\n",
    "!apt update\n",
    "!apt install -y --no-install-recommends \\\n",
    "        build-essential \\\n",
    "        curl \\\n",
    "        git \\\n",
    "        gnupg2 \\\n",
    "        make \\\n",
    "        cmake \\\n",
    "        ffmpeg \\\n",
    "        swig \\\n",
    "        libz-dev \\\n",
    "        unzip \\\n",
    "        zlib1g-dev \\\n",
    "        libglfw3 \\\n",
    "        libglfw3-dev \\\n",
    "        libxrandr2 \\\n",
    "        libxinerama-dev \\\n",
    "        libxi6 \\\n",
    "        libxcursor-dev \\\n",
    "        libgl1-mesa-dev \\\n",
    "        libgl1-mesa-glx \\\n",
    "        libglew-dev \\\n",
    "        libosmesa6-dev \\\n",
    "        lsb-release \\\n",
    "        ack-grep \\\n",
    "        patchelf \\\n",
    "        wget \\\n",
    "        xpra \\\n",
    "        xserver-xorg-dev \\\n",
    "        ffmpeg\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0xE5D5QOioF",
    "outputId": "aa64af22-dc27-4ab0-9320-535f86e64e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/ece421_f2024_A4_folder\n",
      "Cloning into 'ece421fall24_assignments'...\n",
      "remote: Enumerating objects: 83, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
      "remote: Total 83 (delta 24), reused 71 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (83/83), 24.52 MiB | 15.15 MiB/s, done.\n",
      "Resolving deltas: 100% (24/24), done.\n",
      "/content/gdrive/MyDrive/ece421_f2024_A4_folder/ece421fall24_assignments\n"
     ]
    }
   ],
   "source": [
    "#@title ## 0.2 Cloning homework repository\n",
    "\n",
    "#@markdown We do not want to mess up with your changes to the previous\n",
    "#@markdown assignment. So, we create a new directory, named\n",
    "#@markdown `ece421_f2024_A4_folder`, and will clone the latest version of the\n",
    "#@markdown assignments repository there.\n",
    "\n",
    "#@markdown Similar to the previous assignments, you may be promped to restart\n",
    "#@markdown your session.\n",
    "#@markdown We will reset the working directory and reload the required modules\n",
    "#@markdown in the following cells.\n",
    "\n",
    "#@markdown **NOTE:** By executing this cell, the assignment files will be\n",
    "#@markdown downloaded to you google drive. You can click on the folder icon on\n",
    "#@markdown the left panel and navigate through the assignment directory. To\n",
    "#@markdown start editing a file in this assignment, you can navigate to the file\n",
    "#@markdown using the left panel and double click on it.\n",
    "\n",
    "#@markdown **NOTE:** You just need to run this cell once.\n",
    "\n",
    "\n",
    "A4_SYM_PATH = '/content/gdrive/MyDrive/ece421_f2024_A4_folder'\n",
    "%cd $A4_SYM_PATH\n",
    "\n",
    "!git clone https://github.com/erfanmeskar/ece421fall24_assignments.git\n",
    "\n",
    "REPO_PATH = '/content/gdrive/MyDrive/ece421_f2024_A4_folder/ece421fall24_assignments'\n",
    "%cd $REPO_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liAPtVktO6RV",
    "outputId": "4ddcea25-44bb-4ad7-e446-e6b1f9b218a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/ece421_f2024_A4_folder/ece421fall24_assignments/A4\n"
     ]
    }
   ],
   "source": [
    "#@title ## 0.3 Importing the required modules\n",
    "\n",
    "\n",
    "ASSIGNMENT_PATH = '/content/gdrive/MyDrive/ece421_f2024_A4_folder/ece421fall24_assignments/A4'\n",
    "%cd $ASSIGNMENT_PATH\n",
    "\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import train\n",
    "import make_dataset\n",
    "import models\n",
    "import TrainRNN\n",
    "import TrainLSTM\n",
    "import util\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5m0UkPGGvfz"
   },
   "source": [
    "# 1. Adding Problem with Vanilla RNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65JToi6YhdRn",
    "outputId": "46d413e3-f29c-417b-dc64-0ea96d20ee0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyTrainset is an object of our custom made class: <class 'make_dataset.CustomAddingDataset'>\n",
      "Its parent class is: (<class 'torch.utils.data.dataset.Dataset'>,)\n",
      "\n",
      "There are 20 datapoints in the training set\n",
      "There are 4 datapoints in the test set\n",
      "\n",
      "First datapoint in the dataset:\n",
      "(tensor([[0.0015, 0.0000],\n",
      "        [0.2034, 0.0000],\n",
      "        [0.1693, 1.0000],\n",
      "        [0.6331, 0.0000],\n",
      "        [0.4812, 0.0000],\n",
      "        [0.9474, 0.0000],\n",
      "        [0.0327, 0.0000],\n",
      "        [0.4178, 0.0000],\n",
      "        [0.9306, 0.0000],\n",
      "        [0.6696, 1.0000]], device='cuda:0'), tensor([[0.8389]], device='cuda:0'))\n",
      "\n",
      "nFirst datapoint: Input sequence size\n",
      " torch.Size([10, 2])\n",
      "\n",
      "First datapoint: Label size\n",
      " torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "#@title Playing around with a very small dataset\n",
    "\n",
    "#@markdown In the `make_dataset.py` file, we provided you a dataset constructor\n",
    "#@markdown which you will be using for this assignment. By calling the function\n",
    "#@markdown `make_adding_train_val_dataset` from `make_dataset.py`, you can\n",
    "#@markdown create a dataset for our Adding Problem, which will be used to train\n",
    "#@markdown your single layer vanilla RNN and LSTM models.\n",
    "\n",
    "#@markdown You do not need to modify `make_dataset.py`. However, I strongly\n",
    "#@markdown suggest you to skim through this file to understand how the dataset\n",
    "#@markdown was generated. Moreover, there are some comments in this file which\n",
    "#@markdown you may find helpful with implementing the other parts of this\n",
    "#@markdown assignment.\n",
    "\n",
    "#@markdown - Use type() to checkout the type of this dataset. What do you see?\n",
    "\n",
    "#@markdown - How can you find the number of datapoints in this dataset?\n",
    "#@markdown Any custome made subclass of torch.utils.data.Dataset must implement\n",
    "#@markdown a __ len__ () method. You can use that to find the number of\n",
    "#@markdown datapoints in your dataset. Or, simply use the shorthand len(). For\n",
    "#@markdown instance, you can use len(MyDataset) instead of MyDataset.__ len__ ().\n",
    "\n",
    "#@markdown - How can we show the kth datapoint in this dataset? This can be done\n",
    "#@markdown simply by regular indexing that we would use for datastructures like\n",
    "#@markdown list. For instance, MyDataset[k-1] gives you the k_th datapoint.\n",
    "\n",
    "reload(models)\n",
    "reload(train)\n",
    "reload(make_dataset)\n",
    "reload(TrainRNN)\n",
    "reload(TrainLSTM)\n",
    "\n",
    "from make_dataset import make_adding_train_val_dataset\n",
    "\n",
    "ToyTrainSet, ToyValSet = make_adding_train_val_dataset(train_count=20,\n",
    "                                                        val_count=4,\n",
    "                                                        sequence_length=10)\n",
    "\n",
    "\n",
    "print(f\"ToyTrainset is an object of our custom made class: {type(ToyTrainSet)}\")\n",
    "print(f\"Its parent class is: {ToyTrainSet.__class__.__bases__}\")\n",
    "\n",
    "print(f\"\\nThere are {len(ToyTrainSet)} datapoints in the training set\")\n",
    "print(f\"There are {ToyValSet.__len__()} datapoints in the test set\")\n",
    "\n",
    "datapoint_1 = ToyTrainSet[0]\n",
    "print(f\"\\nFirst datapoint in the dataset:\\n{datapoint_1}\")\n",
    "\n",
    "print(f\"\\nnFirst datapoint: Input sequence size\\n {datapoint_1[0].size()}\")\n",
    "print(f\"\\nFirst datapoint: Label size\\n {datapoint_1[1].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5r_f7qODYwkH"
   },
   "outputs": [],
   "source": [
    "#@title ## 1.1 Generating a Datasets with Sequence Length of 10, 25, and 50\n",
    "\n",
    "reload(models)\n",
    "reload(train)\n",
    "reload(make_dataset)\n",
    "reload(TrainRNN)\n",
    "\n",
    "from make_dataset import make_adding_train_val_dataset\n",
    "\n",
    "# Let's Generate a dataset with 10000 training datapoints and 1000 validation\n",
    "# datapoints. Each dapoint has a sequence length of 10\n",
    "Add10Trainset, Add10Valset = make_adding_train_val_dataset(train_count=10000,\n",
    "                                                           val_count=1000,\n",
    "                                                           sequence_length=10)\n",
    "\n",
    "Add25Trainset, Add25Valset = make_adding_train_val_dataset(train_count=10000,\n",
    "                                                           val_count=1000,\n",
    "                                                           sequence_length=25)\n",
    "\n",
    "Add50Trainset, Add50Valset = make_adding_train_val_dataset(train_count=10000,\n",
    "                                                           val_count=1000,\n",
    "                                                           sequence_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "collapsed": true,
    "id": "i2RnXuuDSHEA",
    "outputId": "de57b6d6-502b-448a-9150-4c2dfba01a7b"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-62709f1b6dfc>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mRNNmodel10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss10\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   TrainRNN.train1LayerVanillaRNN(Add10Trainset,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                    \u001b[0mAdd10Valset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                    \u001b[0mRNN_input_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/gdrive/MyDrive/ece421_f2024_A4_folder/ece421fall24_assignments/A4/TrainRNN.py\u001b[0m in \u001b[0;36mtrain1LayerVanillaRNN\u001b[0;34m(train_set, val_set, RNN_input_size, RNN_output_size, RNN_hidden_size, optimizer_name, lr, batch_size, max_epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m                           lr=lr)\n\u001b[1;32m     37\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0moptimizer_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     optimizer = optim.Adam(VRNN_model.parameters(),\n\u001b[0m\u001b[1;32m     39\u001b[0m                           lr=lr)\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor lr must be 1-element\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid learning rate: {lr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'ellipsis'"
     ]
    }
   ],
   "source": [
    "#@title ## 1.2 Find the best **vanilla RNN** model for the dataset with sequence_length=10\n",
    "\n",
    "reload(models)\n",
    "reload(train)\n",
    "reload(make_dataset)\n",
    "reload(TrainRNN)\n",
    "\n",
    "\n",
    "# What should be the value of RNN_input_size and RNN_output_size?\n",
    "RNN_input_size = ...\n",
    "RNN_output_size = ...\n",
    "\n",
    "# Performed a grid search over the hyperparameters below\n",
    "opt_name = 'adam' # let's stick to adam.\n",
    "RNN_hidden_size = ... # choose from 5, or 15\n",
    "learning_rate = ... # choose from 0.1, 0.01, or 0.001\n",
    "\n",
    "RNNmodel10, train_loss10, val_loss10 = \\\n",
    "  TrainRNN.train1LayerVanillaRNN(Add10Trainset,\n",
    "                                   Add10Valset,\n",
    "                                   RNN_input_size,\n",
    "                                   RNN_output_size,\n",
    "                                   RNN_hidden_size,\n",
    "                                   optimizer_name=opt_name,\n",
    "                                   lr=learning_rate,\n",
    "                                   batch_size=64,\n",
    "                                   max_epoch=50)\n",
    "\n",
    "\n",
    "# Plot the loss value for train/test set for each epoch\n",
    "util.plot_loss(train_loss10,\n",
    "          val_loss10,\n",
    "          sequence_len=10,\n",
    "          hidden_size=RNN_hidden_size,\n",
    "          lr=learning_rate,\n",
    "          model_type='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5fQoGUkbj6d"
   },
   "outputs": [],
   "source": [
    "#@title ## 1.3 Find the best **vanilla RNN** model for the dataset with sequence_length=25\n",
    "\n",
    "reload(models)\n",
    "reload(train)\n",
    "reload(make_dataset)\n",
    "reload(TrainRNN)\n",
    "\n",
    "\n",
    "# What should be the value of RNN_input_size and RNN_output_size?\n",
    "RNN_input_size = ...\n",
    "RNN_output_size = ...\n",
    "\n",
    "# Performed a grid search over the hyperparameters below\n",
    "opt_name = 'adam' # let's stick to adam.\n",
    "RNN_hidden_size = ... # choose from 5, or 15\n",
    "learning_rate = ... # choose from 0.1, 0.01, or 0.001\n",
    "\n",
    "RNNmodel25, train_loss25, val_loss25 = \\\n",
    "  TrainRNN.train1LayerVanillaRNN(Add25Trainset,\n",
    "                                   Add25Valset,\n",
    "                                   RNN_input_size,\n",
    "                                   RNN_output_size,\n",
    "                                   RNN_hidden_size,\n",
    "                                   optimizer_name=opt_name,\n",
    "                                   lr=learning_rate,\n",
    "                                   batch_size=64,\n",
    "                                   max_epoch=50)\n",
    "\n",
    "# Plot the loss value for train/test set for each epoch\n",
    "util.plot_loss(train_loss25,\n",
    "          val_loss25,\n",
    "          sequence_len=25,\n",
    "          hidden_size=RNN_hidden_size,\n",
    "          lr=learning_rate,\n",
    "          model_type='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHzs9A83gRG0"
   },
   "outputs": [],
   "source": [
    "#@title ## 1.4 Find the best **vanilla RNN** model for the dataset with sequence_length=50\n",
    "\n",
    "reload(models)\n",
    "reload(train)\n",
    "reload(make_dataset)\n",
    "reload(TrainRNN)\n",
    "\n",
    "\n",
    "# What should be the value of RNN_input_size and RNN_output_size?\n",
    "RNN_input_size = ...\n",
    "RNN_output_size = ...\n",
    "\n",
    "# Performed a grid search over the hyperparameters below\n",
    "opt_name = 'adam' # let's stick to adam.\n",
    "RNN_hidden_size = ... # choose from 5, or 25\n",
    "learning_rate = ... # choose from 0.1, 0.01, or 0.001\n",
    "\n",
    "RNNmodel50, train_loss50, val_loss50 = \\\n",
    "  TrainRNN.train1LayerVanillaRNN(Add50Trainset,\n",
    "                                   Add50Valset,\n",
    "                                   RNN_input_size,\n",
    "                                   RNN_output_size,\n",
    "                                   RNN_hidden_size,\n",
    "                                   optimizer_name=opt_name,\n",
    "                                   lr=learning_rate,\n",
    "                                   batch_size=64,\n",
    "                                   max_epoch=50)\n",
    "\n",
    "# Plot the loss value for train/test set for each epoch\n",
    "util.plot_loss(train_loss50,\n",
    "          val_loss50,\n",
    "          sequence_len=50,\n",
    "          hidden_size=RNN_hidden_size,\n",
    "          lr=learning_rate,\n",
    "          model_type='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RNEZeHlS1Syo"
   },
   "outputs": [],
   "source": [
    "#@title ## 1.5 Find the best **LSTM** model for the dataset with sequence_length=10\n",
    "\n",
    "reload(models)\n",
    "reload(train)\n",
    "reload(make_dataset)\n",
    "reload(TrainLSTM)\n",
    "\n",
    "\n",
    "# What should be the value of LSTM_input_size and LSTM_output_size?\n",
    "LSTM_input_size = ...\n",
    "LSTM_output_size = ...\n",
    "\n",
    "# Performed a grid search over the hyperparameters below\n",
    "LSTM_hidden_size = ... # choose from 2 or 5\n",
    "opt_name = 'adam' # choose from 'sgd' or 'adam'\n",
    "learning_rate = ... # choose from 0.1, 0.01, or 0.001\n",
    "\n",
    "LSTM_model10, lstm_train_loss10, lstm_val_loss10 = \\\n",
    "  TrainLSTM.train1LayerLSTM(Add10Trainset,\n",
    "                                   Add10Valset,\n",
    "                                   LSTM_input_size,\n",
    "                                   LSTM_output_size,\n",
    "                                   LSTM_hidden_size,\n",
    "                                   optimizer_name=opt_name,\n",
    "                                   lr=learning_rate,\n",
    "                                   max_epoch=50,\n",
    "                                   batch_size=64)\n",
    "\n",
    "# Plot the loss value for train/test set for each epoch\n",
    "util.plot_loss(lstm_train_loss10,\n",
    "          lstm_val_loss10,\n",
    "          sequence_len=10,\n",
    "          hidden_size=LSTM_hidden_size,\n",
    "          lr=learning_rate,\n",
    "          model_type='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AY-3K2w1irv-"
   },
   "outputs": [],
   "source": [
    "#@title ## 1.6 Find the best **LSTM** model for the dataset with sequence_length=25\n",
    "\n",
    "reload(models)\n",
    "reload(train)\n",
    "reload(make_dataset)\n",
    "reload(TrainLSTM)\n",
    "\n",
    "\n",
    "# What should be the value of LSTM_input_size and LSTM_output_size?\n",
    "LSTM_input_size = ...\n",
    "LSTM_output_size = ...\n",
    "\n",
    "# Performed a grid search over the hyperparameters below\n",
    "LSTM_hidden_size = ... # choose from 2 or 5\n",
    "opt_name = 'adam' # choose from 'sgd' or 'adam'\n",
    "learning_rate = ... # choose from 0.1, 0.01, or 0.001\n",
    "\n",
    "LSTM_model25, lstm_train_loss25, lstm_val_loss25 = \\\n",
    "  TrainLSTM.train1LayerLSTM(Add25Trainset,\n",
    "                                   Add25Valset,\n",
    "                                   LSTM_input_size,\n",
    "                                   LSTM_output_size,\n",
    "                                   LSTM_hidden_size,\n",
    "                                   optimizer_name=opt_name,\n",
    "                                   lr=learning_rate,\n",
    "                                   max_epoch=50,\n",
    "                                   batch_size=64)\n",
    "\n",
    "# Plot the loss value for train/test set for each epoch\n",
    "util.plot_loss(lstm_train_loss25,\n",
    "          lstm_val_loss25,\n",
    "          sequence_len=25,\n",
    "          hidden_size=LSTM_hidden_size,\n",
    "          lr=learning_rate,\n",
    "          model_type='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BzOKGm3rBE5"
   },
   "outputs": [],
   "source": [
    "#@title ## 1.7 Find the best **LSTM** model for the dataset with sequence_length=50\n",
    "\n",
    "reload(models)\n",
    "reload(train)\n",
    "reload(make_dataset)\n",
    "reload(TrainLSTM)\n",
    "\n",
    "\n",
    "# What should be the value of LSTM_input_size and LSTM_output_size?\n",
    "LSTM_input_size = ...\n",
    "LSTM_output_size = ...\n",
    "\n",
    "# Performed a grid search over the hyperparameters below\n",
    "LSTM_hidden_size = ... # choose from 2 or 5\n",
    "opt_name = 'adam' # choose from 'sgd' or 'adam'\n",
    "learning_rate = ... # choose from 0.1, 0.01, or 0.001\n",
    "\n",
    "LSTM_model50, lstm_train_loss50, lstm_val_loss50 = \\\n",
    "  TrainLSTM.train1LayerLSTM(Add50Trainset,\n",
    "                                   Add50Valset,\n",
    "                                   LSTM_input_size,\n",
    "                                   LSTM_output_size,\n",
    "                                   LSTM_hidden_size,\n",
    "                                   optimizer_name=opt_name,\n",
    "                                   lr=learning_rate,\n",
    "                                   max_epoch=50,\n",
    "                                   batch_size=64)\n",
    "\n",
    "# Plot the loss value for train/test set for each epoch\n",
    "util.plot_loss(lstm_train_loss50,\n",
    "          lstm_val_loss50,\n",
    "          sequence_len=50,\n",
    "          hidden_size=LSTM_hidden_size,\n",
    "          lr=learning_rate,\n",
    "          model_type='lstm')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
